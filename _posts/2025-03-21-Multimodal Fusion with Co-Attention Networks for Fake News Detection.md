---
title: Multimodal Fusion with Co-Attention Networks for Fake News Detection
date: 2025-03-21 14:44:00 +0800 # 时间， 最后为时区北京 +0800
categories: [论文] # 上级文档，下级文档
tags: [虚假新闻检测, 多模态, Co-Attention]     # TAG
---

选择原因：
1. 有代码
2. co-attention
3. 多模态-图片和文字
4. finding of ACL 2021

---

❗baseline 或参考文献中是否有相关研究论文或可复现的代码：


**了解大概研究内容，然后重点关注实验**

---


研究问题：虚假新闻检测（Fake News Detection）

问题重要性或难点：
1. 社交媒体中，图文混合的假新闻比纯文本更具传播欺骗性，传统人工审核效率低，亟需自动化检测方法。
2. 现有多模态假新闻检测方法的核心缺陷在于特征融合不足，仅简单拼接单模态特征，未建模跨模态交互关系。

提出的方法：多模态协同注意力网络（MCAN），通过多个 co-attention 多次融合文本特征和图像的空间域（语义）、频域（物理篡改痕迹） 特征、，以模拟人类阅读图文新闻时的交互行为。

方法特点：
1. 无需社会上下文或辅助任务，降低检测成本
2. 尤其适合与单模态特征较弱时

数据：使用两个真实、公开数据集（短文本 Twitter2017、长文本 Weibo）

实验结果：
1. MCAN 能有效学习多模态特征的跨模态依赖关系。
2. 性能显著优于现有方法（如 MVAE、EANN 等），验证了模型的有效性。


**虚假新闻图像有两种类型**：伪造图像（tampered image）：对图片进行人为修改；误解型图像（misleading image）：图片真实，但是文字的描述虚假。（Exploiting Multi-domain Visual Information for Fake News Detection， ICDM, 2019）

![image.png](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202503212237725.png)

---

## 代码理解


### 模型架构流程图

- 理解MyDataset类 (MCAN.py/MCAN.ipynb中):

- 这是连接原始数据和模型的桥梁

- 重点看__getitem__方法，理解单个样本如何被处理

### 第二步：理解特征提取组件

- 理解图像处理函数:

- process_dct_img函数：了解如何从图像提取DCT特征

- vgg类：了解如何使用预训练VGG提取视觉特征

- 理解文本处理:

- BERT模型如何处理文本输入

- 注意text_input_ids、attention_mask和token_type_ids的作用

### 第三步：注意力机制与特征融合

- 从简单到复杂理解注意力机制:

- 先看multimodal_attention类（基本的点积注意力）

- 再看MultiHeadAttention类（多头注意力）

- 最后看multimodal_fusion_layer类（完整的融合层）

### 第四步：完整模型架构

- 理解主模型:

- NetShareFusion类：这是整个模型的核心

- 关注forward方法，跟踪数据如何从输入流向输出

- 特别注意模态融合的部分

### 第五步：训练过程

- 理解训练逻辑:

- TrainALL类：模型训练的核心

- train_one_time方法：单次训练的具体步骤

- 优化器和学习率调度设置

```
+------------------------------------------------------------------------------------------------------------------------------+
|                                                    数据输入与预处理                                                           |
+------------------------+---------------------------+------------------------+-----------------------------+------------------+
| 原始数据读取            | 图像处理                  | 文本处理                | 数据集构建                   | 批量加载           |
| read_images()          | transforms.Compose()      | BertTokenizer          | MyDataset类                 | DataLoader        |
| get_data()             | process_dct_img()         | encode_plus()          | __getitem__()               | RandomSampler     |
+------------------------+---------------------------+------------------------+-----------------------------+------------------+
                                                             |
                                                             v
+------------------------------------------------------------------------------------------------------------------------------+
|                                                    特征提取模块                                           
|+------------------------+---------------------------+---------------------------------------------------------------------+
| 文本特征提取            | 图像特征提取              | DCT特征提取                                                           |
| BertModel类            | vgg类                     | DctCNN类                                                             |
| self.bert()            | self.vgg()                | self.dct_img()                                                      |
| 输出: [batch, 768]     | 输出: [batch, 4096]       | 输出: [batch, 4096]                                                  |
+------------------------+---------------------------+---------------------------------------------------------------------+
                                                             |
                                                             v
+----------------------------------------------------------------------------------------------------------------------------+
|                                                    特征变换与归一化                                                         |
+------------------------+---------------------------+---------------------------------------------------------------------+
| 文本特征变换            | 图像特征变换              | DCT特征变换                                                         |
| linear_text + BN       | linear_image + BN         | linear_dct + BN                                                    |
| drop_BN_layer("bert")  | drop_BN_layer("vgg")      | drop_BN_layer("dct")                                               |
| 输出: [batch, model_dim]| 输出: [batch, model_dim]  | 输出: [batch, model_dim]                                           |
+------------------------+---------------------------+---------------------------------------------------------------------+
                                                             |
                                                             v
+----------------------------------------------------------------------------------------------------------------------------+
|                                                    多模态注意力融合                                                         |
+---------------------------------------------------------------------------------------------------------------------------+
| 多模态融合层 (multimodal_fusion_layer类)                                                                                   |
|                                                                                                                           |
| +------------------------------------+  +----------------------------------------+  +------------------------+            |
| | 多头注意力机制                      |  | 位置前馈网络                            |  | 特征融合                |            |
| | MultiHeadAttention类              |  | PositionalWiseFeedForward类            |  | fusion_linear           |            |
| | - attention_1: img→text           |  | - feed_forward_1                       |  | torch.cat + 线性变换    |            |
| | - attention_2: text→img           |  | - feed_forward_2                       |  |                        |            |
| +------------------------------------+  +----------------------------------------+  +------------------------+            |
|                                                                                                                           |
| 递进式融合:                                                                                                                |
| 1. output = fusion_layer(image, dct_img)    # 图像+DCT特征融合                                                             |
| 2. output = fusion_layer(output, text)      # 图文特征融合                                                                 |
+---------------------------------------------------------------------------------------------------------------------------+
                                                             |
                                                             v
+----------------------------------------------------------------------------------------------------------------------------+
|                                                    分类与预测                                                               |
+---------------------------------------------------------------------------------------------------------------------------+
| 分类器 (NetShareFusion类)                                                                                                  |
|                                                                                                                           |
| +------------------------------------+  +----------------------------------------+  +------------------------+            |
| | 特征降维                           |  | 最终分类层                              |  | 概率输出                |            |
| | linear1: model_dim → 35           |  | linear2: 35 → num_labels (2)           |  | softmax                 |            |
| | bn_1 + dropout                    |  |                                        |  | y_pred_prob             |            |
| +------------------------------------+  +----------------------------------------+  +------------------------+            |
+---------------------------------------------------------------------------------------------------------------------------+
                                                             |
                                                             v
+-----------------------------------------------------------------------------------------------------------------------------+
|                                                    训练与优化                                                                |
+------------------------+---------------------------+-------------------------+-----------------------------+-----------------+
| 训练循环                | 损失计算                  | 优化器                  | 学习率调度                   | 早停策略           |
| TrainALL.train_one_time| CrossEntropyLoss         | get_optimizer()         | get_scheduler()             | EarlyStopping类   |
| 梯度裁剪               | backward()                | AdamW/AdaBelief/SGD     | 线性预热衰减                 | __call__()        |
+------------------------+---------------------------+-------------------------+-----------------------------+-----------------+
                                                             |
                                                             v
+------------------------------------------------------------------------------------------------------------------------------+
|                                                    模型评估与调优                                                             |
+------------------------+---------------------------+-------------------------+-----------------------------+-----------------+
| 准确率计算              | 精确率/召回率/F1计算       | 报告生成                 | 超参数调优                   | 最佳模型保存     |
| flat_accuracy()        | classification_report     | report(metrics)         | tune.run                    | date_max_test_acc|
| 混淆矩阵生成            | sklearn.metrics           | tabulate                | BayesOptSearch              |                  |
+------------------------+---------------------------+-------------------------+-----------------------------+-----------------+
```



---


## 引言


## 相关工作


### 单模态虚假新闻检测


### 多模态虚假新闻检测




## 模型


模型由特征提取、特征融合、虚假新闻检测三个模块构成

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202503110950447.png)


### 特征提取

所有模态特征通过全连接层统一为 ​256 维，便于后续跨模态对齐和融合。

在Twitter数据集上，由于数据量较小，​**VGG-19和BERT的参数被冻结**以防止过拟合；在Weibo数据集上则进行微调。

#### 图片-频域（frequency-domain）特征

捕捉图像在**物理层面**的篡改痕迹（如重压缩伪影、周期性噪声等）。

通过 ​**离散余弦变换（DCT）​** 将图像从空间域转换到频域（具体方法参考 Qi et al., 2019）

1. **CNN网络设计**：
    - ​**主干网络**：
        - 包含 ​**3个卷积块**​（每个块含卷积层 + 批归一化 + ReLU）。
        - 插入 ​**最大池化层** 降低维度。
    - ​**多分支结构**：
        - 采用 ​**Inception V3** 的多分支架构（Szegedy et al., 2016），增强局部特征提取能力。
    - ​**末端处理**：
        - 通过 ​**最大池化层** 和 ​**卷积块** 进一步压缩特征。
        - 添加 ​**全连接层（f-fc）​**，使用 ​**ReLU** 激活函数。
2. ​**输出**：
    - 生成 ​**256维特征向量** RF​∈R256×1，表示图像的频域物理特征。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202503110951682.png)



假新闻图像可能是低质量的，比如在社交媒体上多次上传和压缩而导致的块状效果（block effect）；被篡改的图像会有人为操作的痕迹。

重新压缩的图像和篡改的图像通常在频域中表现出**周期性**，具有捕获空间（spatial）结构特征能力的 CNN 可以很容易地表征出这些特征。作者设计了基于 CNN 的网络，自动捕获假新闻图像在频域中的特性。（Exploiting Multi-domain Visual Information for Fake News Detection， ICDM, 2019）
![image.png](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202503212104268.png)


#### 图片-空间域（Spatial-Domain）特征

捕捉图像在**语义层面**的视觉特征（如物体、场景等）。

​**主干网络**：使用预训练的 ​**VGG-19** 网络（Simonyan & Zisserman, 2014）。
​**特征提取层**：
* 提取 VGG-19 ​**最后一个卷积层之前的第二层**​（即 `conv5_4` 层）的输出。
* 添加一个 ​**全连接层（s-fc）​**，使用 ​**ReLU** 激活函数。
​**输出**：
* 生成 ​**256维特征向量** RS​∈R256×1，表示图像的空间域语义特征。


假新闻图像在像素域（即空域）也有一些明显的特性。假新闻发布者倾向于利用图像来吸引并误导读者，以实现新闻的快速传播。因此假新闻了图像通常有视觉冲击（visual impact）和情感挑衅（emotional provocations）。（Exploiting Multi-domain Visual Information for Fake News Detection， ICDM, 2019）
![image.png](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202503212105109.png)


#### 文本特征

推文中的文本视为一系列单词，每个单词由 BERT 中的词汇表来 tokenize，

利用 BERT 模型（Devlin et al., 2018）获取推文的**聚合序列表征**❓，以此作为文本特征。

通过全连接层的 RELU 激活函数，将文本特征 $R_T$ 重设大小为 $256×1$ 维度


### 特征融合

方法对比

| ​**方法**      | ​**特征交互方式**               | ​**缺陷**             |
| ------------ | ------------------------- | ------------------- |
| 简单拼接（MCAN-A） | 直接拼接 $R_S​$、$R_F$、$R_T$ ​ | 忽略模态间关联，无法建模复杂依赖    |
| 单层注意力（如VQA）  | 单次跨模态注意力                  | 浅层交互，难以捕捉多层次关系      |
| ​**MCAN**    | 多层协同注意力 + 渐进融合            | 深度建模跨模态依赖，提升细粒度推理能力 |

#### 协同注意力机制

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202503110951030.png)

协同注意力块（Co-attention block）

两个特征之间有**两次协同注意力层**

❓问题：
1. 特征融合的顺序是否有影响？
2. 融合了多头注意力（multi-head attention）吗？

结构：
* 分支 1：模态 A 的特征作为 Query，模块 B 的特征作为 Key/Value。$R_{C_{F->S}}=R_S+MA(Q_S,K_F,V_F)$
* 分支 2：模态 B 的特征作为 Query，模块 A 的特征作为 Key/Value。$R_{C_{S->F}}=R_F+MA(Q_F,K_S,V_S)$
* $R_C^{(1)}=Concat(R_{C_{F->S}}, R_{C_{S->F}})W_C^{(1)}$

co-attention能模拟人阅读时，看完图片看文字，重复这个过程，融合图像和文本中的信息。

当以图像特征作为 Q，以文本特征作为 K 和 V，表示：根据图像特征引导或决定从文本特征中提取的信息，就像是，看完图片后，会更加主义去看文本中和图片相关的内容，或则说以图像信息为主导。相反的，则以文本信息为主导。


> 本文中的协同注意力机制，参考自：ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks, **NIPS 19**，[jiasenlu/vilbert_beta](https://github.com/jiasenlu/vilbert_beta)
> 
> 每两个特征进行两次 co-attention，原因：更深的去融合多模态特征
> 


![image.png](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202503111104890.png)

> VQA 图片问答场景下的 co-attention
> ![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202503201614288.png)


## 实验

### 数据集
C. Boididou, S. Papadopoulos, D. Dang-Nguyen, G. Boato, and Y. Kompatsiaris. 2016. Verifying multimedia use at mediaeval 2016. In MediaEval 2016 Workshop.
Zhiwei Jin, Juan Cao, Han Guo, Yongdong Zhang, and Jiebo Luo. 2017. Multimodal fusion with recurrent neural networks for rumor detection on microblogs. In Proceedings of the 25th ACM international conference on Multimedia, pages 795–816.

微博数据集中，真实的新闻已由中国的权威新闻社（新华社）验证。假新闻的官方谣言揭穿了微博系统。

每个数据集中的推文都包含文本，附加图像/视频和社交环境信息。在这项工作中，我们专注于文本和图像信息。因此，我们使用视频和推文删除了没有文字或图像的推文。

在 Twitter 数据集中，其余数据共享了 512 张图像。在预处理微博数据集时，我们使用的步骤与工作中的步骤相似（Jin 等，2017）。我们保留与这两个数据集上的基准相同的数据拆分方案。表 1 列出了两个数据集的详细统计信息。

|           | twitter | weibo |
| --------- | ------- | ----- |
| fake news | 8199    | 4211  |
| real news | 6681    | 3639  |
| images    | 512     | 7850  |
| 文本最大长度    | 25      | 160   |


### 实验设置

特征提取模块抽取的文本特征、图片的频域特征、图片的空间域特征的维度均为 256

设置待融合的特征维度 $d=256$，协同注意力层的层数 $m=4$，这个是什么？❓ $d_{ff}=512$

设置最终的隐藏层输出的 `p-fs` 的维度为 35。

训练 twitter 数据集时，VGG-19 和 **multilingual-cased-BERT** 的**参数被冻结**。训练 weibo 数据集时，采用 **bert-base-chinese**。

模型训练设置 epoch=100，设置早停规则

使用 Adam 和 AdaBelief 作为优化器

模型的最优化参数通过网格搜索（grid search）确定，选择标准为 accuracy

baseline 中的超参数选用各自研究中的超参数值


### baseline

下选择单模态模型和多模态模型作为 baseline

| 模型名        | 模型描述                                  | 来源  |
| ---------- | ------------------------------------- | --- |
| Text       | BERT+MCAN 中的决策网络                      |     |
| Spatial    | VGG-19+MCAN 中的决策网络                    |     |
| Freq       | 保留MCAN 中处理频域特征模块                      |     |
| VQA        | 根据图片回答问题的模型+单层LSTM❓                   |     |
| NeuralTalk | 获取图片概括的深层循环网络，图文联合表征由 RNN 在每个时间步平均值得到 |     |
| att-RNN    | 利用局部网络融合文本、视觉和社会上下文特征                 |     |
| EANN       | 删除特定时间特征的、基于对抗的神经网络                   |     |
| MVAE       | 使用 VAE 和二分类器学习图文表征                    |     |
| MCAN-A     | 移除多模态特征融合部分，将三个特征简单的拼接用于预测            |     |


### 性能比较

MCAN 模型在两个数据集上的结果全面优于 baselines

对比 MCAN-A 和单模态模型，大部分情况 MCAN-A 优于单模态模型，说明添加特征有利于提升模型性能

对比 MCAN 和 MCAN-A 模型，本文采用的 co-attention 优于普通的特征连接方法
​
​Twitter 数据局限：推文短且 70%集中于单一事件，导致单模态模型（如 Text 和 Spatial）易过拟合，所以 BERT 和 VGG-19 未微调。

​Weibo 数据优势：文本更长、分布均衡，微调后的 BERT 和 VGG-19 表现优异，MCAN 通过多模态融合进一步提升性能。


![image.png](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202503110953318.png)


### 消融分析
#### 定量分析

评估模型中每个组件的有效性。**既有移除特征，也有移除模型的组件吗？**

|      |                       |
| ---- | --------------------- |
| ALL  | MCAN 模型               |
| -F   | 移除频域特征                |
| -A   | 移除 co-attention 层     |
| -T   | 移除文本特征                |
| -S   | 移除空间域特征               |
| -F-A | 移除频域与和 co-attention 层 |

MCAN 优于 MCAN-F，表明频域信息确实有助于检测假新闻。

在 Twitter 数据集上，文本特征对整个模型的贡献小于视觉特征的贡献，而 weibo 数据集中的情况则相反。这是由于问题不平衡以及 Twitter 数据集上推文的平均长度较小，这降低了文本特征的性能。

在微博数据集上，删除一个或两个组件，MCAN 的性能并不能像 Twitter 数据集上的明显下降。如第 4.4 节所述，这受益于平衡数据分布以及微调 BERT 和 VGG-19 的稳定性。

![image.png](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202503110954569.png)


#### 定性分析

- ​**方法**：  
    使用t-SNE技术对MCAN的简化版本（**MCAN-A**，仅特征拼接）和完整模型（**MCAN**）在Weibo测试集上的特征分布进行降维可视化，对比两类新闻（真实/虚假）的特征可分性。
    
- ​**结果**：
    - ​**MCAN-A**​（图6a）：特征分布存在重叠区域，部分样本易被误分类，表明简单拼接多模态特征无法充分捕捉跨模态依赖。
    - ​**完整MCAN**​（图6b）：特征呈现显著分离，真实与虚假新闻的分布边界清晰，说明协同注意力层通过深度交互增强了特征的判别性。
- ​**结论**：  
    协同注意力机制通过迭代融合多模态信息，有效提升了特征的区分能力，支持了MCAN在定量实验中的优越性能。


❓虽然直观上，a 图展示两种样本交叉较多，但是 b 图中也有交叉。是否需要通过更精确的计算数量来确定？

![image.png](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202503110954757.png)


### 案例研究

这四个案例表明，单模态模型（无论是基于文本还是图像）在假新闻检测中存在局限性。MCAN 模型通过利用多模态特征，能够更准确地识别假新闻，尤其在单模态信息不足以判断真实性的情况下，多模态特征的结合提供了更高的判断置信度。

Figure 7 左图，MCAN 模型通过结合文本和图像的多模态特征，正确识别出这是假新闻，而仅依赖文本的模型未能检测到。

Figure 7 右图，MCAN 模型通过多模态特征正确识别出这是假新闻，而仅依赖文本的模型未能检测到。

Figure 8 左图，MCAN 模型通过结合文本和图像的多模态特征，正确识别出这是假新闻，而仅依赖图像的模型未能检测到。

Figure 8 右图，MCAN 模型通过结合文本和图像的多模态特征，正确识别出这是假新闻，而仅依赖图像的模型未能检测到。

| ![image.png](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202503110955607.png) | ![image.png](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202503110956279.png) |
| ------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------ |


## 结论

在这项工作中，我们提出了一个新颖的多模态协同注意力网络（MCAN），以应对融合多模式（文本和视觉）功能的挑战以进行虚假新闻检测。

我们利用三个不同的子网络从文本，空间域和频域中提取特征。然后，这三个特征通过堆叠共同注意层深刻融合，这是受人类行为的启发。当人们阅读带有图像，图像和文本的新闻时，将被读取一次或多次，并不断融合在大脑中。

在两个公共基准数据集上进行虚假新闻检测的实验验证了 MCAN 的有效性，结果表明 MCAN 的表现优于当前的最新方法。

将来，我们计划将 MCAN 基于共同注意的融合方法扩展到其他假新闻研究，例如假新闻传播。









